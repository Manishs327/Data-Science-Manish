{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"DQN_Agent_Arch1.ipynb","provenance":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ANQakq_d-rfS"},"source":["### Cab-Driver Agent"]},{"cell_type":"code","metadata":{"id":"iYMlZPrI-rfg","executionInfo":{"status":"ok","timestamp":1621150087029,"user_tz":-330,"elapsed":3100,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["# Importing libraries\n","import numpy as np\n","import random\n","import math\n","from collections import deque\n","import collections\n","import pickle\n","import time\n","import pylab\n","\n","# for building DQN model\n","from keras import layers\n","from keras import Sequential\n","from keras.layers import Dense, Activation, Flatten\n","from keras.optimizers import Adam\n","\n","# for plotting graphs\n","import matplotlib.pyplot as plt\n","\n","# Import the environment\n","from Env import CabDriver"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pLeaj5On-rfi"},"source":["#### Defining Time Matrix"]},{"cell_type":"code","metadata":{"id":"eU0zKHof-rfj","executionInfo":{"status":"ok","timestamp":1621150087031,"user_tz":-330,"elapsed":3094,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["# Loading the time matrix provided\n","Time_matrix = np.load(\"TM.npy\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEdHMdrv-rfk","executionInfo":{"status":"ok","timestamp":1621150087032,"user_tz":-330,"elapsed":3090,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}},"outputId":"b40de402-6f57-48fb-a1d3-4400901e631d"},"source":["# Checking Time Matrix for max, min and mean time values\n","\n","print(type(Time_matrix))\n","print(Time_matrix.max())\n","print(Time_matrix.min())\n","print(Time_matrix.mean())\n","print(Time_matrix.var())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","11.0\n","0.0\n","3.0542857142857143\n","7.93705306122449\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o5JGXiEj-rfm"},"source":["#### Tracking the state-action pairs for checking convergence\n"]},{"cell_type":"code","metadata":{"id":"o67Adly7-rfm","executionInfo":{"status":"ok","timestamp":1621150087033,"user_tz":-330,"elapsed":3086,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["def encode_track_state(state):\n","    return ('-'.join(str(e) for e in state))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"deiO7VpA-rfn","executionInfo":{"status":"ok","timestamp":1621150087033,"user_tz":-330,"elapsed":3082,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["def encode_track_action(action):\n","    return ('-'.join(str(e) for e in action))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"zND84brc-rfn","executionInfo":{"status":"ok","timestamp":1621150087034,"user_tz":-330,"elapsed":3080,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["# Tracking q-values for all possible moves from position 4 at the start of the episode\n","\n","def initialise_tracking_states(episodes):\n","    sample_q_values = [('4-0-0', '1-2'), ('4-0-0', '2-1'), ('4-0-0', '1-3'), ('4-0-0', '3-1'), ('4-0-0', '1-4'), ('4-0-0', '4-1'), ('4-0-0', '1-5'), ('4-0-0', '5-1'), ('4-0-0', '2-3'), \n","                       ('4-0-0', '3-2'), ('4-0-0', '2-4'), ('4-0-0', '4-2'), ('4-0-0', '2-5'), ('4-0-0', '5-2'), ('4-0-0', '3-4'), ('4-0-0', '4-3'), ('4-0-0', '3-5'), ('4-0-0', '5-3'), \n","                       ('4-0-0', '4-5'), ('4-0-0', '5-4'), ('4-0-0', '0-0')]    \n","    for q_value in sample_q_values:\n","        state = q_value[0]\n","        action = q_value[1]\n","        States_track[state][action] = []"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"PEtgYnaq-rfo","executionInfo":{"status":"ok","timestamp":1621150087035,"user_tz":-330,"elapsed":3077,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["# Function to save q-value for tracking states-action pair\n","\n","def save_tracking_states(curr_state, curr_action, curr_q_value):\n","    for state in States_track.keys():\n","        if state == curr_state:\n","            for action in States_track[state].keys():\n","                if action == curr_action:\n","                    States_track[state][action].append(curr_q_value)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6Zg-5OL-rfp","executionInfo":{"status":"ok","timestamp":1621150087035,"user_tz":-330,"elapsed":3071,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["# Defining a function to save the Q-dictionary as a pickle file\n","\n","def save_obj(obj, name ):\n","    with open(name + '.pkl', 'wb') as f:\n","        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DMRgMrW-rfp"},"source":["### Agent Class\n","\n","If you are using this framework, you need to fill the following to complete the following code block:\n","1. State and Action Size\n","2. Hyperparameters\n","3. Create a neural-network model in function 'build_model()'\n","4. Define epsilon-greedy strategy in function 'get_action()'\n","5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n","6. Complete the 'train_model()' function with following logic:\n","   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n","      - Initialise your input and output batch for training the model\n","      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n","      - Get Q(s', a) values from the last trained model\n","      - Update the input batch as your encoded state and output batch as your Q-values\n","      - Then fit your DQN model using the updated input and output batch."]},{"cell_type":"code","metadata":{"id":"zpR9bpA3-rfq","executionInfo":{"status":"ok","timestamp":1621150087036,"user_tz":-330,"elapsed":3069,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        # Define size of state and action\n","        self.state_size = state_size\n","        self.action_size = action_size\n","\n","        # Write here: Specify you hyper parameters for the DQN\n","        self.discount_factor = 0.95\n","        self.learning_rate = 0.01\n","        self.epsilon = 1\n","        self.epsilon_max = 1\n","        self.epsilon_decay = -0.0005 #for 15k\n","        #self.epsilon_decay = -0.00015 #for 20k\n","        self.epsilon_min = 0.00001\n","        \n","        self.batch_size = 32\n","\n","        # create replay memory using deque\n","        self.memory = deque(maxlen=2000)\n","\n","        # Initialize the value of the states tracked\n","        self.states_tracked = []\n","        \n","        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n","        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n","\n","        # create main model and target model\n","        self.model = self.build_model()\n","\n","    # approximate Q function using Neural Network\n","    def build_model(self):\n","        \"\"\"\n","        Function that takes in the agent and constructs the network\n","        to train it\n","        @return model\n","        @params agent\n","        \"\"\"\n","        input_shape = self.state_size\n","        model = Sequential()\n","        # Write your code here: Add layers to your neural nets       \n","        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n","        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n","        # the output layer: output is of size num_actions\n","        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","        model.summary\n","        return model\n","\n","    def get_action(self, state, possible_actions_index, actions):\n","        \"\"\"\n","        get action in a state according to an epsilon-greedy approach\n","        possible_actions_index, actions are the 'ride requests' that teh driver got.\n","        \"\"\"        \n","        # get action from model using epsilon-greedy policy\n","        # Decay in ε after each episode       \n","        if np.random.rand() <= self.epsilon:\n","            # explore: choose a random action from the ride requests\n","            return random.choice(possible_actions_index)\n","        else:\n","            # choose the action with the highest q(s, a)\n","            # the first index corresponds to the batch size, so\n","            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n","            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n","\n","            # Use the model to predict the Q_values.\n","            q_value = self.model.predict(state)\n","\n","            # truncate the array to only those actions that are part of the ride  requests.\n","            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n","\n","            return possible_actions_index[np.argmax(q_vals_possible)]\n","\n","    def append_sample(self, state, action_index, reward, next_state, done):\n","        \"\"\"appends the new agent run output to replay buffer\"\"\"\n","        self.memory.append((state, action_index, reward, next_state, done))\n","        \n","    # pick samples randomly from replay memory (with batch_size) and train the network\n","    def train_model(self):\n","        \"\"\" \n","        Function to train the model on eacg step run.\n","        Picks the random memory events according to batch size and \n","        runs it through the network to train it.\n","        \"\"\"\n","        if len(self.memory) > self.batch_size:\n","            # Sample batch from the memory\n","            mini_batch = random.sample(self.memory, self.batch_size)\n","            # initialise two matrices - update_input and update_output\n","            update_input = np.zeros((self.batch_size, self.state_size))\n","            update_output = np.zeros((self.batch_size, self.state_size))\n","            actions, rewards, done = [], [], []\n","\n","            # populate update_input and update_output and the lists rewards, actions, done\n","            for i in range(self.batch_size):\n","                state, action, reward, next_state, done_boolean = mini_batch[i]\n","                update_input[i] = env.state_encod_arch1(state)     \n","                actions.append(action)\n","                rewards.append(reward)\n","                update_output[i] = env.state_encod_arch1(next_state)\n","                done.append(done_boolean)\n","\n","            # predict the target q-values from states s\n","            target = self.model.predict(update_input)\n","            # target for q-network\n","            target_qval = self.model.predict(update_output)\n","\n","\n","            # update the target values\n","            for i in range(self.batch_size):\n","                if done[i]:\n","                    target[i][actions[i]] = rewards[i]\n","                else: # non-terminal state\n","                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n","            # model fit\n","            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n","            \n","    def save_tracking_states(self):\n","        # Use the model to predict the q_value of the state we are tacking.\n","        q_value = self.model.predict(self.track_state)\n","        \n","        # Grab the q_value of the action index that we are tracking.\n","        self.states_tracked.append(q_value[0][2])\n","        \n","    def save_test_states(self):\n","        # Use the model to predict the q_value of the state we are tacking.\n","        q_value = self.model.predict(self.track_state)\n","        \n","        # Grab the q_value of the action index that we are tracking.\n","        self.states_test.append(q_value[0][2])\n","\n","    def get_model_weights(self):\n","        return self.model.get_weights()\n","        \n","#     def save(self, name):\n","#         with open(name, 'wb') as file:  \n","#             pickle.dump(self.model, file,pickle.HIGHEST_PROTOCOL)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"isQ5qRzP-rfy","executionInfo":{"status":"ok","timestamp":1621150087037,"user_tz":-330,"elapsed":3066,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["Episodes = 15000"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zX9_yoGQ-rf1"},"source":["### DQN block"]},{"cell_type":"code","metadata":{"id":"-FiwYdLm-rf2","executionInfo":{"status":"ok","timestamp":1621150087747,"user_tz":-330,"elapsed":3772,"user":{"displayName":"Manish Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjM2viKvPwwUAB6X5i1dCr4i_-YLHU7VB2hyADFqw=s64","userId":"02674790647366382560"}}},"source":["episode_time = 24*30         #30 days before which car has to be recharged\n","n_episodes = 8000\n","m = 5\n","t = 24\n","d = 7\n","\n","# Invoke Env class\n","env = CabDriver()\n","action_space, state_space, state = env.reset()\n","\n","# Set up state and action sizes.\n","state_size = m+t+d\n","action_size = len(action_space)\n","\n","# Invoke agent class\n","agent = DQNAgent(action_size=action_size, state_size=state_size)\n","\n","# to store rewards in each episode\n","rewards_per_episode, episodes = [], []\n","\n","# Rewards for state [0,0,0] being tracked.\n","rewards_init_state = []"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vC_dT-qG-rf2","outputId":"707aed30-1426-43d0-c8fd-56542dc88f18"},"source":["%%time\n","start_time = time.time()\n","score_tracked = []\n","\n","for episode in range(n_episodes):\n","\n","    done = False\n","    score = 0\n","    track_reward = False\n","\n","    # reset at the start of each episode\n","    env = CabDriver()\n","    action_space, state_space, state = env.reset()\n","    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n","    initial_state = env.state_init\n","\n","\n","    total_time = 0  # Total time driver rode in this episode\n","    while not done:\n","        # 1. Get a list of the ride requests driver got.\n","        possible_actions_indices, actions = env.requests(state)\n","        # 2. Pick epsilon-greedy action from possible actions for the current state.\n","        action = agent.get_action(state, possible_actions_indices, actions)\n","\n","        # 3. Evaluate your reward and next state\n","        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n","        # 4. Total time driver rode in this episode\n","        total_time += step_time\n","        if (total_time > episode_time):\n","            # if ride does not complete in stipulated time skip\n","            # it and move to next episode.\n","            done = True\n","        else:\n","            # 5. Append the experience to the memory\n","            agent.append_sample(state, action, reward, next_state, done)\n","            # 6. Train the model by calling function agent.train_model\n","            agent.train_model()\n","            # 7. Keep a track of rewards, Q-values, loss\n","            score += reward\n","            state = next_state\n","\n","    # store total reward obtained in this episode\n","    rewards_per_episode.append(score)\n","    episodes.append(episode)\n","    \n","\n","    # epsilon decay\n","    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n","\n","    # every 10 episodes:\n","    if ((episode + 1) % 10 == 0):\n","        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n","                                                                         score,\n","                                                                         len(agent.memory),\n","                                                                         agent.epsilon, total_time))\n","    # Save the Q_value of the state, action pair we are tracking\n","    if ((episode + 1) % 5 == 0):\n","        agent.save_tracking_states()\n","\n","    # Total rewards per episode\n","    score_tracked.append(score)\n","\n","#     if(episode % 1000 == 0):\n","#         print(\"Saving Model {}\".format(episode))\n","#         agent.save(name=\"model_weights.pkl\")\n","        \n","    if (episode % 1000 == 0):\n","        print(\"Episode:\", episode + 1, \"  score:\", score, \"  memory length:\", len(agent.memory), \"  epsilon:\", agent.epsilon)\n","        pylab.plot(episodes, rewards_per_episode)\n","#         pylab.savefig(\"cardriver_dqn_\" + str(score) + \".png\")\n","#         agent.save(\"cardriver_dqn_\" + str(score) + \".h5\")\n","        #agent.save_model_graph(str(score))\n","        # this is the only one required for assignment. this will create pickle file\n","        save_obj(agent.get_model_weights(), \"cardriver_dqn_\" + str(score))\n","    \n","elapsed_time = time.time() - start_time\n","print(elapsed_time)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Episode: 1   score: 64.0   memory length: 165   epsilon: 0.99999\n","episode 9, reward -238.0, memory_length 1471, epsilon 0.9955001547284723 total_time 723.0\n","episode 19, reward 78.0, memory_length 2000, epsilon 0.9905350769930761 total_time 723.0\n","episode 29, reward -336.0, memory_length 2000, epsilon 0.9855947626861951 total_time 722.0\n","episode 39, reward -141.0, memory_length 2000, epsilon 0.9806790882997144 total_time 721.0\n","episode 49, reward -243.0, memory_length 2000, epsilon 0.9757879309415182 total_time 730.0\n","episode 59, reward 232.0, memory_length 2000, epsilon 0.9709211683324178 total_time 721.0\n","episode 69, reward -117.0, memory_length 2000, epsilon 0.9660786788030947 total_time 729.0\n","episode 79, reward -332.0, memory_length 2000, epsilon 0.9612603412910584 total_time 721.0\n","episode 89, reward 142.0, memory_length 2000, epsilon 0.9564660353376199 total_time 722.0\n","episode 99, reward -187.0, memory_length 2000, epsilon 0.9516956410848808 total_time 734.0\n","episode 109, reward 59.0, memory_length 2000, epsilon 0.9469490392727365 total_time 725.0\n","episode 119, reward -244.0, memory_length 2000, epsilon 0.9422261112358942 total_time 725.0\n","episode 129, reward 100.0, memory_length 2000, epsilon 0.9375267389009072 total_time 728.0\n","episode 139, reward -65.0, memory_length 2000, epsilon 0.9328508047832221 total_time 727.0\n","episode 149, reward 52.0, memory_length 2000, epsilon 0.9281981919842428 total_time 722.0\n","episode 159, reward 221.0, memory_length 2000, epsilon 0.9235687841884068 total_time 726.0\n","episode 169, reward -94.0, memory_length 2000, epsilon 0.918962465660278 total_time 724.0\n","episode 179, reward 63.0, memory_length 2000, epsilon 0.9143791212416534 total_time 728.0\n","episode 189, reward 135.0, memory_length 2000, epsilon 0.9098186363486838 total_time 728.0\n","episode 199, reward 46.0, memory_length 2000, epsilon 0.9052808969690094 total_time 730.0\n","episode 209, reward -8.0, memory_length 2000, epsilon 0.9007657896589091 total_time 725.0\n","episode 219, reward -139.0, memory_length 2000, epsilon 0.8962732015404654 total_time 732.0\n","episode 229, reward -13.0, memory_length 2000, epsilon 0.891803020298741 total_time 723.0\n","episode 239, reward -171.0, memory_length 2000, epsilon 0.8873551341789723 total_time 726.0\n","episode 249, reward -180.0, memory_length 2000, epsilon 0.8829294319837746 total_time 727.0\n","episode 259, reward 36.0, memory_length 2000, epsilon 0.8785258030703623 total_time 727.0\n","episode 269, reward 153.0, memory_length 2000, epsilon 0.8741441373477834 total_time 736.0\n","episode 279, reward 93.0, memory_length 2000, epsilon 0.8697843252741666 total_time 727.0\n","episode 289, reward 54.0, memory_length 2000, epsilon 0.8654462578539829 total_time 723.0\n","episode 299, reward -136.0, memory_length 2000, epsilon 0.8611298266353209 total_time 721.0\n","episode 309, reward 164.0, memory_length 2000, epsilon 0.8568349237071754 total_time 722.0\n","episode 319, reward 445.0, memory_length 2000, epsilon 0.8525614416967494 total_time 722.0\n","episode 329, reward 180.0, memory_length 2000, epsilon 0.84830927376677 total_time 723.0\n","episode 339, reward 208.0, memory_length 2000, epsilon 0.8440783136128177 total_time 722.0\n","episode 349, reward -126.0, memory_length 2000, epsilon 0.8398684554606681 total_time 721.0\n","episode 359, reward 7.0, memory_length 2000, epsilon 0.8356795940636483 total_time 723.0\n","episode 369, reward 144.0, memory_length 2000, epsilon 0.8315116247000052 total_time 726.0\n","episode 379, reward 439.0, memory_length 2000, epsilon 0.8273644431702872 total_time 725.0\n","episode 389, reward -91.0, memory_length 2000, epsilon 0.8232379457947406 total_time 721.0\n","episode 399, reward 136.0, memory_length 2000, epsilon 0.819132029410716 total_time 721.0\n","episode 409, reward -9.0, memory_length 2000, epsilon 0.8150465913700896 total_time 721.0\n","episode 419, reward 234.0, memory_length 2000, epsilon 0.8109815295366979 total_time 734.0\n","episode 429, reward 199.0, memory_length 2000, epsilon 0.8069367422837833 total_time 726.0\n","episode 439, reward 216.0, memory_length 2000, epsilon 0.8029121284914538 total_time 724.0\n","episode 449, reward -300.0, memory_length 2000, epsilon 0.7989075875441549 total_time 721.0\n","episode 459, reward 216.0, memory_length 2000, epsilon 0.7949230193281545 total_time 723.0\n","episode 469, reward 343.0, memory_length 2000, epsilon 0.7909583242290396 total_time 729.0\n","episode 479, reward 106.0, memory_length 2000, epsilon 0.7870134031292261 total_time 723.0\n","episode 489, reward -89.0, memory_length 2000, epsilon 0.7830881574054811 total_time 723.0\n","episode 499, reward 540.0, memory_length 2000, epsilon 0.7791824889264571 total_time 728.0\n","episode 509, reward 78.0, memory_length 2000, epsilon 0.7752963000502389 total_time 722.0\n","episode 519, reward 412.0, memory_length 2000, epsilon 0.7714294936219019 total_time 721.0\n","episode 529, reward -128.0, memory_length 2000, epsilon 0.7675819729710842 total_time 728.0\n","episode 539, reward 199.0, memory_length 2000, epsilon 0.763753641909569 total_time 728.0\n","episode 549, reward 82.0, memory_length 2000, epsilon 0.7599444047288803 total_time 724.0\n","episode 559, reward 153.0, memory_length 2000, epsilon 0.7561541661978903 total_time 722.0\n","episode 569, reward 243.0, memory_length 2000, epsilon 0.7523828315604384 total_time 726.0\n","episode 579, reward 172.0, memory_length 2000, epsilon 0.7486303065329623 total_time 726.0\n","episode 589, reward 59.0, memory_length 2000, epsilon 0.7448964973021404 total_time 722.0\n","episode 599, reward 1.0, memory_length 2000, epsilon 0.7411813105225479 total_time 730.0\n","episode 609, reward 370.0, memory_length 2000, epsilon 0.7374846533143217 total_time 722.0\n","episode 619, reward 387.0, memory_length 2000, epsilon 0.733806433260839 total_time 721.0\n","episode 629, reward 234.0, memory_length 2000, epsilon 0.7301465584064071 total_time 726.0\n","episode 639, reward 198.0, memory_length 2000, epsilon 0.7265049372539636 total_time 722.0\n","episode 649, reward 95.0, memory_length 2000, epsilon 0.7228814787627905 total_time 728.0\n","episode 659, reward 82.0, memory_length 2000, epsilon 0.7192760923462365 total_time 733.0\n","episode 669, reward 72.0, memory_length 2000, epsilon 0.7156886878694535 total_time 721.0\n","episode 679, reward 152.0, memory_length 2000, epsilon 0.7121191756471427 total_time 722.0\n","episode 689, reward 54.0, memory_length 2000, epsilon 0.7085674664413126 total_time 725.0\n","episode 699, reward 132.0, memory_length 2000, epsilon 0.7050334714590482 total_time 729.0\n","episode 709, reward 273.0, memory_length 2000, epsilon 0.7015171023502909 total_time 730.0\n","episode 719, reward 339.0, memory_length 2000, epsilon 0.6980182712056295 total_time 722.0\n","episode 729, reward 276.0, memory_length 2000, epsilon 0.6945368905541035 total_time 722.0\n","episode 739, reward 286.0, memory_length 2000, epsilon 0.6910728733610152 total_time 724.0\n","episode 749, reward 230.0, memory_length 2000, epsilon 0.6876261330257543 total_time 721.0\n","episode 759, reward 523.0, memory_length 2000, epsilon 0.684196583379633 total_time 728.0\n","episode 769, reward 317.0, memory_length 2000, epsilon 0.6807841386837313 total_time 722.0\n","episode 779, reward 369.0, memory_length 2000, epsilon 0.6773887136267543 total_time 723.0\n","episode 789, reward 397.0, memory_length 2000, epsilon 0.6740102233228988 total_time 722.0\n","episode 799, reward 120.0, memory_length 2000, epsilon 0.670648583309731 total_time 727.0\n","episode 809, reward 234.0, memory_length 2000, epsilon 0.6673037095460755 total_time 731.0\n","episode 819, reward 313.0, memory_length 2000, epsilon 0.6639755184099142 total_time 721.0\n","episode 829, reward 220.0, memory_length 2000, epsilon 0.6606639266962953 total_time 722.0\n","episode 839, reward 261.0, memory_length 2000, epsilon 0.6573688516152534 total_time 729.0\n","episode 849, reward 221.0, memory_length 2000, epsilon 0.6540902107897397 total_time 725.0\n","episode 859, reward 240.0, memory_length 2000, epsilon 0.6508279222535631 total_time 722.0\n","episode 869, reward 249.0, memory_length 2000, epsilon 0.64758190444934 total_time 724.0\n","episode 879, reward 28.0, memory_length 2000, epsilon 0.6443520762264566 total_time 723.0\n","episode 889, reward 254.0, memory_length 2000, epsilon 0.6411383568390387 total_time 727.0\n","episode 899, reward 267.0, memory_length 2000, epsilon 0.6379406659439346 total_time 723.0\n","episode 909, reward 148.0, memory_length 2000, epsilon 0.6347589235987051 total_time 722.0\n","episode 919, reward 151.0, memory_length 2000, epsilon 0.631593050259626 total_time 723.0\n","episode 929, reward 828.0, memory_length 2000, epsilon 0.6284429667796988 total_time 724.0\n","episode 939, reward 288.0, memory_length 2000, epsilon 0.6253085944066726 total_time 730.0\n","episode 949, reward 552.0, memory_length 2000, epsilon 0.6221898547810748 total_time 724.0\n","episode 959, reward 655.0, memory_length 2000, epsilon 0.6190866699342522 total_time 725.0\n","episode 969, reward 554.0, memory_length 2000, epsilon 0.6159989622864221 total_time 732.0\n","episode 979, reward 558.0, memory_length 2000, epsilon 0.6129266546447325 total_time 729.0\n","episode 989, reward 477.0, memory_length 2000, epsilon 0.6098696702013323 total_time 721.0\n","episode 999, reward 329.0, memory_length 2000, epsilon 0.6068279325314512 total_time 723.0\n","Episode: 1001   score: 211.0   memory length: 2000   epsilon: 0.6065245944060363\n","episode 1009, reward 222.0, memory_length 2000, epsilon 0.6038013655914889 total_time 722.0\n","episode 1019, reward 215.0, memory_length 2000, epsilon 0.6007898937171146 total_time 725.0\n","episode 1029, reward 600.0, memory_length 2000, epsilon 0.5977934416213744 total_time 732.0\n","episode 1039, reward 475.0, memory_length 2000, epsilon 0.5948119343928097 total_time 725.0\n","episode 1049, reward 447.0, memory_length 2000, epsilon 0.5918452974935846 total_time 722.0\n","episode 1059, reward 504.0, memory_length 2000, epsilon 0.5888934567576223 total_time 726.0\n","episode 1069, reward 719.0, memory_length 2000, epsilon 0.5859563383887504 total_time 724.0\n","episode 1079, reward 396.0, memory_length 2000, epsilon 0.5830338689588568 total_time 731.0\n","episode 1089, reward 338.0, memory_length 2000, epsilon 0.5801259754060536 total_time 729.0\n","episode 1099, reward 415.0, memory_length 2000, epsilon 0.5772325850328504 total_time 725.0\n","episode 1109, reward 792.0, memory_length 2000, epsilon 0.5743536255043372 total_time 722.0\n","episode 1119, reward 431.0, memory_length 2000, epsilon 0.5714890248463761 total_time 728.0\n","episode 1129, reward 734.0, memory_length 2000, epsilon 0.5686387114438011 total_time 725.0\n","episode 1139, reward 959.0, memory_length 2000, epsilon 0.5658026140386287 total_time 722.0\n","episode 1149, reward 676.0, memory_length 2000, epsilon 0.5629806617282764 total_time 726.0\n","episode 1159, reward 767.0, memory_length 2000, epsilon 0.5601727839637891 total_time 722.0\n","episode 1169, reward 744.0, memory_length 2000, epsilon 0.5573789105480766 total_time 725.0\n","episode 1179, reward 279.0, memory_length 2000, epsilon 0.5545989716341581 total_time 729.0\n","episode 1189, reward 398.0, memory_length 2000, epsilon 0.5518328977234156 total_time 723.0\n","episode 1199, reward 469.0, memory_length 2000, epsilon 0.5490806196638577 total_time 728.0\n","episode 1209, reward 522.0, memory_length 2000, epsilon 0.5463420686483893 total_time 721.0\n","episode 1219, reward 540.0, memory_length 2000, epsilon 0.5436171762130925 total_time 721.0\n","episode 1229, reward 394.0, memory_length 2000, epsilon 0.5409058742355145 total_time 721.0\n","episode 1239, reward 635.0, memory_length 2000, epsilon 0.5382080949329644 total_time 724.0\n","episode 1249, reward 568.0, memory_length 2000, epsilon 0.5355237708608195 total_time 726.0\n","episode 1259, reward 875.0, memory_length 2000, epsilon 0.5328528349108379 total_time 726.0\n","episode 1269, reward 663.0, memory_length 2000, epsilon 0.5301952203094819 total_time 722.0\n","episode 1279, reward 343.0, memory_length 2000, epsilon 0.5275508606162479 total_time 725.0\n","episode 1289, reward 574.0, memory_length 2000, epsilon 0.5249196897220061 total_time 724.0\n","episode 1299, reward 657.0, memory_length 2000, epsilon 0.5223016418473468 total_time 728.0\n","episode 1309, reward 541.0, memory_length 2000, epsilon 0.519696651540937 total_time 724.0\n","episode 1319, reward 432.0, memory_length 2000, epsilon 0.5171046536778833 total_time 728.0\n","episode 1329, reward 644.0, memory_length 2000, epsilon 0.514525583458104 total_time 725.0\n","episode 1339, reward 788.0, memory_length 2000, epsilon 0.5119593764047093 total_time 722.0\n","episode 1349, reward 738.0, memory_length 2000, epsilon 0.5094059683623896 total_time 722.0\n","episode 1359, reward 505.0, memory_length 2000, epsilon 0.5068652954958104 total_time 730.0\n","episode 1369, reward 699.0, memory_length 2000, epsilon 0.5043372942880178 total_time 736.0\n","episode 1379, reward 795.0, memory_length 2000, epsilon 0.50182190153885 total_time 723.0\n","episode 1389, reward 574.0, memory_length 2000, epsilon 0.49931905436335716 total_time 722.0\n","episode 1399, reward 580.0, memory_length 2000, epsilon 0.49682869019022974 total_time 721.0\n","episode 1409, reward 594.0, memory_length 2000, epsilon 0.49435074676023355 total_time 734.0\n","episode 1419, reward 919.0, memory_length 2000, epsilon 0.4918851621246539 total_time 726.0\n","episode 1429, reward 477.0, memory_length 2000, epsilon 0.4894318746437464 total_time 724.0\n","episode 1439, reward 460.0, memory_length 2000, epsilon 0.4869908229851962 total_time 725.0\n","episode 1449, reward 711.0, memory_length 2000, epsilon 0.48456194612258474 total_time 723.0\n","episode 1459, reward 744.0, memory_length 2000, epsilon 0.48214518333386397 total_time 723.0\n","episode 1469, reward 1009.0, memory_length 2000, epsilon 0.47974047419983834 total_time 727.0\n","episode 1479, reward 548.0, memory_length 2000, epsilon 0.4773477586026542 total_time 721.0\n","episode 1489, reward 705.0, memory_length 2000, epsilon 0.474966976724297 total_time 724.0\n","episode 1499, reward 818.0, memory_length 2000, epsilon 0.47259806904509577 total_time 724.0\n","episode 1509, reward 622.0, memory_length 2000, epsilon 0.4702409763422352 total_time 724.0\n","episode 1519, reward 941.0, memory_length 2000, epsilon 0.4678956396882749 total_time 724.0\n","episode 1529, reward 617.0, memory_length 2000, epsilon 0.4655620004496764 total_time 729.0\n","episode 1539, reward 868.0, memory_length 2000, epsilon 0.46324000028533724 total_time 721.0\n","episode 1549, reward 869.0, memory_length 2000, epsilon 0.4609295811451323 total_time 724.0\n","episode 1559, reward 707.0, memory_length 2000, epsilon 0.4586306852684627 total_time 723.0\n","episode 1569, reward 1025.0, memory_length 2000, epsilon 0.4563432551828119 total_time 722.0\n","episode 1579, reward 362.0, memory_length 2000, epsilon 0.4540672337023085 total_time 723.0\n","episode 1589, reward 819.0, memory_length 2000, epsilon 0.45180256392629703 total_time 725.0\n","episode 1599, reward 509.0, memory_length 2000, epsilon 0.4495491892379152 total_time 724.0\n","episode 1609, reward 521.0, memory_length 2000, epsilon 0.4473070533026783 total_time 724.0\n","episode 1619, reward 871.0, memory_length 2000, epsilon 0.4450761000670712 total_time 722.0\n","episode 1629, reward 869.0, memory_length 2000, epsilon 0.4428562737571469 total_time 723.0\n","episode 1639, reward 653.0, memory_length 2000, epsilon 0.44064751887713194 total_time 725.0\n","episode 1649, reward 970.0, memory_length 2000, epsilon 0.4384497802080393 total_time 724.0\n","episode 1659, reward 855.0, memory_length 2000, epsilon 0.4362630028062879 total_time 730.0\n","episode 1669, reward 459.0, memory_length 2000, epsilon 0.43408713200232857 total_time 722.0\n","episode 1679, reward 500.0, memory_length 2000, epsilon 0.43192211339927816 total_time 725.0\n","episode 1689, reward 1008.0, memory_length 2000, epsilon 0.4297678928715586 total_time 725.0\n","episode 1699, reward 640.0, memory_length 2000, epsilon 0.4276244165635446 total_time 721.0\n","episode 1709, reward 938.0, memory_length 2000, epsilon 0.42549163088821684 total_time 728.0\n","episode 1719, reward 738.0, memory_length 2000, epsilon 0.4233694825258223 total_time 728.0\n","episode 1729, reward 862.0, memory_length 2000, epsilon 0.4212579184225415 total_time 722.0\n","episode 1739, reward 1208.0, memory_length 2000, epsilon 0.4191568857891617 total_time 722.0\n","episode 1749, reward 704.0, memory_length 2000, epsilon 0.4170663320997578 total_time 722.0\n","episode 1759, reward 956.0, memory_length 2000, epsilon 0.4149862050903786 total_time 727.0\n","episode 1769, reward 882.0, memory_length 2000, epsilon 0.4129164527577405 total_time 737.0\n","episode 1779, reward 432.0, memory_length 2000, epsilon 0.41085702335792745 total_time 729.0\n","episode 1789, reward 896.0, memory_length 2000, epsilon 0.40880786540509717 total_time 721.0\n","episode 1799, reward 869.0, memory_length 2000, epsilon 0.4067689276701942 total_time 726.0\n","episode 1809, reward 827.0, memory_length 2000, epsilon 0.40474015917966877 total_time 724.0\n","episode 1819, reward 838.0, memory_length 2000, epsilon 0.4027215092142031 total_time 727.0\n","episode 1829, reward 810.0, memory_length 2000, epsilon 0.4007129273074429 total_time 724.0\n","episode 1839, reward 999.0, memory_length 2000, epsilon 0.39871436324473586 total_time 721.0\n","episode 1849, reward 727.0, memory_length 2000, epsilon 0.3967257670618763 total_time 723.0\n","episode 1859, reward 826.0, memory_length 2000, epsilon 0.3947470890438561 total_time 724.0\n","episode 1869, reward 798.0, memory_length 2000, epsilon 0.3927782797236218 total_time 725.0\n","episode 1879, reward 1126.0, memory_length 2000, epsilon 0.3908192898808378 total_time 724.0\n","episode 1889, reward 556.0, memory_length 2000, epsilon 0.388870070540656 total_time 723.0\n","episode 1899, reward 307.0, memory_length 2000, epsilon 0.38693057297249134 total_time 727.0\n","episode 1909, reward 842.0, memory_length 2000, epsilon 0.3850007486888037 total_time 730.0\n","episode 1919, reward 1014.0, memory_length 2000, epsilon 0.3830805494438854 total_time 730.0\n","episode 1929, reward 558.0, memory_length 2000, epsilon 0.38116992723265536 total_time 731.0\n","episode 1939, reward 755.0, memory_length 2000, epsilon 0.3792688342894587 total_time 723.0\n","episode 1949, reward 1126.0, memory_length 2000, epsilon 0.3773772230868729 total_time 728.0\n","episode 1959, reward 633.0, memory_length 2000, epsilon 0.37549504633451936 total_time 722.0\n","episode 1969, reward 673.0, memory_length 2000, epsilon 0.37362225697788115 total_time 721.0\n","episode 1979, reward 834.0, memory_length 2000, epsilon 0.37175880819712703 total_time 726.0\n","episode 1989, reward 816.0, memory_length 2000, epsilon 0.3699046534059402 total_time 727.0\n","episode 1999, reward 1040.0, memory_length 2000, epsilon 0.3680597462503545 total_time 731.0\n","Episode: 2001   score: 605.0   memory length: 2000   epsilon: 0.3678757623770306\n","episode 2009, reward 1063.0, memory_length 2000, epsilon 0.3662240406075948 total_time 724.0\n","episode 2019, reward 748.0, memory_length 2000, epsilon 0.3643974905849244 total_time 736.0\n","episode 2029, reward 923.0, memory_length 2000, epsilon 0.3625800505184978 total_time 724.0\n","episode 2039, reward 656.0, memory_length 2000, epsilon 0.3607716749722184 total_time 722.0\n","episode 2049, reward 1225.0, memory_length 2000, epsilon 0.3589723187366037 total_time 726.0\n","episode 2059, reward 900.0, memory_length 2000, epsilon 0.35718193682765376 total_time 727.0\n","episode 2069, reward 1273.0, memory_length 2000, epsilon 0.3554004844857278 total_time 723.0\n","episode 2079, reward 936.0, memory_length 2000, epsilon 0.35362791717442443 total_time 725.0\n","episode 2089, reward 925.0, memory_length 2000, epsilon 0.35186419057946866 total_time 723.0\n","episode 2099, reward 738.0, memory_length 2000, epsilon 0.3501092606076035 total_time 723.0\n","episode 2109, reward 1023.0, memory_length 2000, epsilon 0.3483630833854885 total_time 721.0\n","episode 2119, reward 1011.0, memory_length 2000, epsilon 0.34662561525860197 total_time 724.0\n","episode 2129, reward 1149.0, memory_length 2000, epsilon 0.34489681279015044 total_time 728.0\n","episode 2139, reward 700.0, memory_length 2000, epsilon 0.34317663275998195 total_time 723.0\n","episode 2149, reward 834.0, memory_length 2000, epsilon 0.3414650321635064 total_time 721.0\n","episode 2159, reward 1045.0, memory_length 2000, epsilon 0.33976196821061944 total_time 724.0\n","episode 2169, reward 764.0, memory_length 2000, epsilon 0.3380673983246338 total_time 722.0\n","episode 2179, reward 873.0, memory_length 2000, epsilon 0.336381280141214 total_time 728.0\n","episode 2189, reward 997.0, memory_length 2000, epsilon 0.33470357150731744 total_time 723.0\n","episode 2199, reward 1066.0, memory_length 2000, epsilon 0.3330342304801412 total_time 721.0\n","episode 2209, reward 676.0, memory_length 2000, epsilon 0.33137321532607245 total_time 729.0\n","episode 2219, reward 1092.0, memory_length 2000, epsilon 0.3297204845196459 total_time 726.0\n","episode 2229, reward 1245.0, memory_length 2000, epsilon 0.32807599674250526 total_time 727.0\n","episode 2239, reward 927.0, memory_length 2000, epsilon 0.32643971088237056 total_time 726.0\n","episode 2249, reward 1127.0, memory_length 2000, epsilon 0.32481158603200994 total_time 726.0\n","episode 2259, reward 1096.0, memory_length 2000, epsilon 0.32319158148821747 total_time 725.0\n","episode 2269, reward 1044.0, memory_length 2000, epsilon 0.3215796567507951 total_time 721.0\n","episode 2279, reward 676.0, memory_length 2000, epsilon 0.31997577152154044 total_time 725.0\n","episode 2289, reward 891.0, memory_length 2000, epsilon 0.31837988570323916 total_time 727.0\n","episode 2299, reward 1166.0, memory_length 2000, epsilon 0.3167919593986629 total_time 724.0\n","episode 2309, reward 1481.0, memory_length 2000, epsilon 0.3152119529095711 total_time 721.0\n","episode 2319, reward 972.0, memory_length 2000, epsilon 0.3136398267357194 total_time 721.0\n","episode 2329, reward 984.0, memory_length 2000, epsilon 0.31207554157387146 total_time 723.0\n","episode 2339, reward 505.0, memory_length 2000, epsilon 0.3105190583168168 total_time 731.0\n","episode 2349, reward 973.0, memory_length 2000, epsilon 0.30897033805239293 total_time 727.0\n","episode 2359, reward 1272.0, memory_length 2000, epsilon 0.3074293420625127 total_time 723.0\n","episode 2369, reward 950.0, memory_length 2000, epsilon 0.3058960318221959 total_time 726.0\n","episode 2379, reward 883.0, memory_length 2000, epsilon 0.30437036899860687 total_time 725.0\n","episode 2389, reward 1238.0, memory_length 2000, epsilon 0.3028523154500953 total_time 729.0\n","episode 2399, reward 1248.0, memory_length 2000, epsilon 0.30134183322524366 total_time 731.0\n","episode 2409, reward 1103.0, memory_length 2000, epsilon 0.29983888456191743 total_time 723.0\n","episode 2419, reward 625.0, memory_length 2000, epsilon 0.29834343188632195 total_time 723.0\n","episode 2429, reward 1030.0, memory_length 2000, epsilon 0.2968554378120623 total_time 721.0\n","episode 2439, reward 640.0, memory_length 2000, epsilon 0.2953748651392093 total_time 726.0\n","episode 2449, reward 792.0, memory_length 2000, epsilon 0.2939016768533689 total_time 727.0\n","episode 2459, reward 846.0, memory_length 2000, epsilon 0.2924358361247571 total_time 723.0\n","episode 2469, reward 745.0, memory_length 2000, epsilon 0.2909773063072796 total_time 726.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UMcSaHqr-rf4"},"source":["agent.save(name=\"model_weights.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnOk0vJ6-rf5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m0AS3We4-rf5"},"source":["### Tracking Convergence"]},{"cell_type":"code","metadata":{"id":"0k_dkLYV-rf6"},"source":["agent.states_tracked"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpdCizEW-rf6"},"source":["state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qDYVJld-rf6"},"source":["# Plot the Q-Value convergence for state action pairs\n","\n","plt.figure(0, figsize=(16,7))\n","plt.title('Q_value for state [0,0,0]  action (0,2)')\n","xaxis = np.asarray(range(0, len(agent.states_tracked)))\n","plt.semilogy(xaxis,np.asarray(agent.states_tracked))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R29TESQG-rf7"},"source":["# Track rewards per episode.\n","\n","score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYbbgc72-rf7"},"source":["plt.figure(0, figsize=(16,7))\n","plt.title('Rewards per episode')\n","xaxis = np.asarray(range(0, len(score_tracked_sample)))\n","plt.plot(xaxis,np.asarray(score_tracked_sample))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uf-F8Kk0-rf8"},"source":["#### Epsilon-decay sample function"]},{"cell_type":"markdown","metadata":{"id":"prMn_gTv-rf8"},"source":["<div class=\"alert alert-block alert-info\">\n","Try building a similar epsilon-decay function for your model.\n","</div>"]},{"cell_type":"code","metadata":{"id":"DTamH_1h-rf8"},"source":["import numpy as np\n","time = np.arange(0,15000)\n","epsilon = []\n","for i in range(0,15000):\n","    epsilon.append((1 - 0.00001) * np.exp(-0.0005 * i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S22Z9PW4-rf9"},"source":["plt.plot(time, epsilon)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ButbkqZe-rf9"},"source":[""],"execution_count":null,"outputs":[]}]}